{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b549fd",
   "metadata": {},
   "source": [
    "# 4.0 Ensemble Model Training\n",
    "\n",
    "## What is Ensemble Model Training?\n",
    "\n",
    "<div>\n",
    "<img src=\"static/ensemble-learning-bagging.png\" alt=\"ANOVA\" width=\"700\"/>.    \n",
    "</div>\n",
    "\n",
    "[Image source](https://www.ibm.com/think/topics/ensemble-learning)\n",
    "\n",
    "In our Anvil workflow, our ensemble learning process at a high-level is:\n",
    "1. the training data is bootstrapped, aka randomly sampled with replacement\n",
    "2. a user-selected model, e.g. `LightGBM Regressor`, `ChemProp Regressor`, etc. is trained on the bootstrapped data\n",
    "3. Steps 1 and 2 repeat for a user-designated `n` number of models\n",
    "4. the predictions of each trained model are combined, e.g. averaged together\n",
    "\n",
    "## Why Train an Ensemble of Models?\n",
    "\n",
    "For complex problems, like predicting ADMET profiles of compounds where data is sparse, training an ensemble of models increases the robustness and accuracy of the model predictions, especially compared to relying on a single model.  \n",
    "\n",
    "Further, since you're taking an average of the preditions across `n` number of models, you are also able to calculate the standard deviation or **uncertainty** of your predictions. This is essential for evaluating how **reliable** the ADME model is for real world applications and performing **active learning** with your model.\n",
    "\n",
    "> If I'm relying on my model's predictions to pick active compounds, how off can my predictions be?\n",
    "\n",
    "> What degree of error does my model have?\n",
    "\n",
    "> What compounds would be most informative to assay next?\n",
    "\n",
    "### Requirements\n",
    "As in `02_Training_Models.ipynb`, you will need:\n",
    "\n",
    "1. A dataset that has been processed with `01_Curate_ChEMBL_Data.ipynb`.  \n",
    "2. A `YAML` file with instructions for Anvil and specifically for ensemble model training. We will show you how to create this file in this notebook.\n",
    "\n",
    "## Overview\n",
    "This notebook will walk you through how to train an ensemble of models with the Anvil workflow with the same CYP3A4 data used in `02_Training_Models.ipynb`.\n",
    "\n",
    "## Create the YAML file\n",
    "As in `02_Training_Models.ipynb`, we will use a `YAML` file containing all the necessary information to train the ensemble. The only difference from the usual anvil recipe is the `ensemble` section.  \n",
    "\n",
    "In the below example, we will be training a **5-model** ensemble of `LGBM` regressors with the CYP3A4 ChEMBL data.  \n",
    "\n",
    "```yaml\n",
    "# This spection specifies the input data\n",
    "data:\n",
    "  # Specify the dataset file\n",
    "  resource: ../01_Data_Curation/processed_data/processed_CYP3A4_inhibition.csv\n",
    "  type: intake\n",
    "  input_col: OPENADMET_SMILES\n",
    "  # Specify each (1+) of the target columns, or the column that you're trying to predict\n",
    "  target_cols:\n",
    "  - OPENADMET_LOGAC50\n",
    "  dropna: true\n",
    "\n",
    "# Additional metadata\n",
    "metadata:\n",
    "  authors: Your Name\n",
    "  email: youremail@email.com\n",
    "  biotargets:\n",
    "  - CYP3A4\n",
    "  build_number: 0\n",
    "  description: basic regression using a LightGBM model\n",
    "  driver: sklearn\n",
    "  name: lgbm_pchembl\n",
    "  tag: openadmet-chembl\n",
    "  tags:\n",
    "  - openadmet\n",
    "  - test\n",
    "  - pchembl\n",
    "  version: v1\n",
    "\n",
    "# Section specifying training procedure\n",
    "procedure:\n",
    "# Featurization specification\n",
    "  feat:\n",
    "    # Using concatenated features, which combines multiple featurizers\n",
    "    # here we use DescriptorFeaturizer and FingerprintFeaturizer for 2D RDKit descriptors and ECFP4 fingerprints\n",
    "    # See openadmet.models.features \n",
    "    type: FeatureConcatenator\n",
    "    # Add parameters for the featurizer. Full description of the featurizer options are in Section 5.\n",
    "    params:\n",
    "      featurizers:\n",
    "        DescriptorFeaturizer:\n",
    "          descr_type: \"desc2d\"\n",
    "        FingerprintFeaturizer:\n",
    "          fp_type: \"ecfp:4\"\n",
    "  \n",
    "  # Model specification\n",
    "  model:\n",
    "    # Indicate model type\n",
    "    # See openadmet.models.architecture for all model types\n",
    "    type: LGBMRegressorModel\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      alpha: 0.005\n",
    "      learning_rate: 0.05\n",
    "      n_estimators: 500\n",
    "\n",
    "  # Ensemble specification\n",
    "  ensemble:\n",
    "    type: CommitteeRegressor\n",
    "    n_models: 5\n",
    "    calibration_method: scaling-factor\n",
    "\n",
    "  # Specify data splits\n",
    "  split:\n",
    "    # Specify how data will be split\n",
    "    # See openadmet.models.split\n",
    "    type: ShuffleSplitter\n",
    "    # Specify split parameters\n",
    "    params:\n",
    "      random_state: 42\n",
    "      train_size: 0.7\n",
    "      val_size: 0.1 # Validation set is needed for uncertainty calibration\n",
    "      test_size: 0.2 # If you want to compare tree-based models with Dl models later, the test sizes should match\n",
    "    \n",
    "  # Specify training configuration\n",
    "  train:\n",
    "    # Specify the trainer, here SKLearnBasicTrainer as model has an sklearn interface\n",
    "    # could also use SKLearnGridSearchTrainer for hyperparameter tuning\n",
    "    type: SKLearnBasicTrainer\n",
    "\n",
    "\n",
    "# Section specifying report generation\n",
    "report:\n",
    "  # Configure evaluation\n",
    "  eval:\n",
    "  # Generate regression metrics\n",
    "  - type: RegressionMetrics\n",
    "    params: {}\n",
    "  # Generate regression plots & do cross validation\n",
    "  - type: SKLearnRepeatedKFoldCrossValidation\n",
    "    params:\n",
    "      axes_labels:\n",
    "      - True pAC50\n",
    "      - Predicted pAC50\n",
    "      max_val: 10\n",
    "      min_val: 3\n",
    "      pXC50: true\n",
    "      n_splits: 5\n",
    "      n_repeats: 5\n",
    "      title: True vs Predicted pAC50 on test set\n",
    "  # Generate uncertainty metrics\n",
    "  - type: UncertaintyMetrics\n",
    "    params:\n",
    "      bins: 100\n",
    "      resolution: 99\n",
    "      scaled: True\n",
    "  # Generate uncertainty calibration plot\n",
    "  - type: UncertaintyPlots\n",
    "    params: {}\n",
    "```\n",
    "\n",
    "The command for running anvil is exactly the same as it was before!\n",
    "\n",
    "```bash\n",
    "    openadmet anvil --recipe-path anvil_ensemble.yaml --output-dir ensemble\n",
    "```\n",
    "\n",
    "We have already run this for you and have provided the results of the ensemble model training in `ensemble/`.  \n",
    "\n",
    "**If training deep learning models, we highly recommend training on GPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d68f4e",
   "metadata": {},
   "source": [
    "With the ensemble training model output, you'll notice that there is now a new plot: `OPENADMET_LOGAC50_uncertainty-calibration-plot.png`.\n",
    "\n",
    "<div>\n",
    "<img src=\"ensemble/OPENADMET_LOGAC50_uncertainty-calibration-plot.png\" alt=\"uncertainty\" width=\"500\"/>.  \n",
    "</div>\n",
    "\n",
    "This plot helps visualize how miscalibrated or \"off\" the ensemble's predicted uncertainty intervals (derived from the calculated standard deviation) are.  \n",
    "\n",
    "If our model perfectly predicted the uncertainty intervals - in other words, if the error between $y_{predicted}$ and $y_{actual}$ always fell within the predicted uncertainty intervals - then the blue line would fall precisely along the dotted black line.\n",
    "\n",
    "<div>\n",
    "<img src=\"static/uncertainty_labelled.png\" alt=\"uncertainty labelled\" width=\"400\"/>.  \n",
    "</div>\n",
    "\n",
    "If the blue area was above the black line in the **underconfidence** zone, this would mean that our model's uncertainty intervals are too large or, our model is underestimating uncertainty.\n",
    "\n",
    "When the blue area is below the black line in the **overconfidence** zone, this means the uncertainty intervals are too narrow or, our model is overestimating uncertainty.  \n",
    "\n",
    "We can see our blue line does not completely fall along the dotted black line, but the shaded blue area represents the area of miscalibration, aka where our uncertainty estimation is inaccurate, is reasonably small (`=0.02`).  \n",
    "\n",
    "This is because in our ensemble model training Anvil workflow, we applied a scaling factor to recalibrate our uncertainty estimations to minimize the miscalibration area. This is known as **uncertainty calibration** and helps **empircally** adjust the width of your error bars to match the distribution expected from a validation set. This is done using the [Uncertainty Toolbox](https://github.com/uncertainty-toolbox/uncertainty-toolbox) package. Learn more by reading the documentation there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87381d98",
   "metadata": {},
   "source": [
    "~ End of `04_Ensemble_Model_Training` ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
