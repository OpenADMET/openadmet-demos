{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fb9470",
   "metadata": {},
   "source": [
    "# 2.1 Training models with Anvil\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"anvil_diagram.png\" alt=\"Anvil diagram\" width=\"500\"/>  \n",
    "</div>\n",
    "\n",
    "### Background\n",
    "\n",
    "Anvil is our primary infrastructure for model training and evaluation, built to support scalable, reproducible, and rigorous development of ADMET prediction models. Recognizing that building the best models requires training many variants, ensuring their reproducibility, and enabling robust performance comparisons, Anvil centers around a YAML-based recipe system. These recipes allow users to specify model architectures and training procedures in a standardized, shareable format—minimizing code duplication while supporting both deep learning and traditional machine learning approaches.  \n",
    "\n",
    "Designed with both internal and external engagement in mind, Anvil aims to lower the barrier for outside users to adopt and fine-tune models by offering simple, transparent workflows. Long-term, it will serve as a foundation for broader community involvement and model reuse.\n",
    "\n",
    "### Requirements\n",
    "To run Anvil, you need:\n",
    "1. A dataset that has been processed with `1.1_Curating_external_datasets.ipynb`.  \n",
    "2. A `YAML` file with instructions for Anvil. We will show you how to create this file in this notebook.\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "This notebook will walk you through how to run the Anvil model training workflow with human pregnane X receptor (PXR) data processed and cleaned in previous notebooks.\n",
    "\n",
    "## 2. Creating the YAML file\n",
    "The heart of an anvil run is in its `YAML` configuration file. Here we specify nearly everything needed to:\n",
    "- load data\n",
    "- preprocess it\n",
    "- split the data appropriately into train/validation/test\n",
    "- featurize according to model selection\n",
    "- train the model\n",
    "- and, finally, validate on the test set (which generates performance metrics and plots)  \n",
    "\n",
    "We will walkthrough two `YAML` files: one for training a traditional machine learning model (`2.1_anvil_lgbm.yaml`) and one for training a deep learning model (`2.1_anvil_chemprop.yaml`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7a193",
   "metadata": {},
   "source": [
    "## 3. Training a traditional machine learning LightGBM \n",
    "\n",
    "Here is a `YAML` file for training a LightGBM (LGBM) model. We are using the previously curated PXR data from ChEMBL. Be sure to read through the comments (in green) to understand each field.  \n",
    "\n",
    "1. At a minimum, ensure `resource`, `input_col`, and `target_cols` are specified to match your dataset, as these will vary per dataset\n",
    "2. The `procedure` section may not need much modification, especially if not tweaking parameters, but look it over to make sure it’s sensible\n",
    "\n",
    "```yaml \n",
    "# This spection specifies the input data\n",
    "data:\n",
    "  # Specify where anvil will run (inputs, outputs, etc.)\n",
    "  anvil_dir: demos/anvil_models/lgbm\n",
    "  # Specify the dataset file\n",
    "  resource: processed_data/processed_PXR_chembl.parquet\n",
    "  # TODO: add description of intake\n",
    "  type: intake\n",
    "  cat_entry: null\n",
    "  # Specify input column containing SMILES\n",
    "  input_col: OPENADMET_CANONICAL_SMILES\n",
    "  # Specify each (1+) of the target columns, or the column that you're trying to predict\n",
    "  target_cols:\n",
    "  - OPENADMET_LOGAC50\n",
    "\n",
    "# Additional metadata\n",
    "metadata:\n",
    "  authors: Cynthia Xu\n",
    "  biotargets:\n",
    "  - PXR\n",
    "  build_number: 0\n",
    "  description: basic regression using a LightGBM model\n",
    "  driver: sklearn\n",
    "  email: cynthia.xu@omsf.io\n",
    "  name: lgbm_pchembl\n",
    "  tag: lgbm-PXR-chembl\n",
    "  tags:\n",
    "  - openadmet\n",
    "  - test\n",
    "  - pchembl\n",
    "  version: v1\n",
    "\n",
    "# This section specifies the training procedure\n",
    "procedure:\n",
    "  # Featurization specification\n",
    "  feat:\n",
    "    # Using a concatenated descriptor and fingerprint as feature vectors\n",
    "    # See openadmet.models.features\n",
    "    type: FeatureConcatenator\n",
    "    # Add parameters for the featurizer. Full description of the featurizer options are in Section 5.\n",
    "    params:\n",
    "      featurizers:\n",
    "        DescriptorFeaturizer:\n",
    "          descr_type: \"desc2d\"\n",
    "        FingerprintFeaturizer:\n",
    "          fp_type: \"ecfp:4\"\n",
    "  \n",
    "  # This section specifies the model and hyperparameters\n",
    "  model:\n",
    "    # Indicate model type, here an LGBM regressor\n",
    "    # See openadmet.models.architecture for all model types\n",
    "    type: LGBMRegressorModel\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      alpha: 0.005\n",
    "      learning_rate: 0.05\n",
    "\n",
    "  # Specify data splits\n",
    "  split:\n",
    "    # Specify how data will be split\n",
    "    # See openadmet.models.split\n",
    "    type: ShuffleSplitter\n",
    "    # Specify split parameters\n",
    "    params:\n",
    "      random_state: 42\n",
    "      train_size: 0.7\n",
    "      val_size: 0.0 # For LGBM, no validation set is needed as we are not doing model selection\n",
    "      test_size: 0.3\n",
    "    \n",
    "  # Specify training configuration\n",
    "  train:\n",
    "    # Specify the trainer, here GridSearch trainer for a hyperparameter sweep \n",
    "    # See openadmet.models.trainer\n",
    "    type: SKLearnGridSearchTrainer\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      param_grid:\n",
    "        n_estimators: [200, 500]\n",
    "        alpha: [0.005, 0.01]\n",
    "        learning_rate: [0.05]\n",
    "\n",
    "# This section specifies the report generation\n",
    "report:\n",
    "  # Configure evaluation\n",
    "  eval:\n",
    "  # Generate regression metrics\n",
    "  - type: RegressionMetrics\n",
    "    params: {}\n",
    "  # Generate regression plots from cross validation\n",
    "  - type: SKLearnRepeatedKFoldCrossValidation\n",
    "    params:\n",
    "      axes_labels:\n",
    "      - True pAC50\n",
    "      - Predicted pAC50\n",
    "      max_val: 10\n",
    "      min_val: 3\n",
    "      pXC50: true\n",
    "      title: True vs Predicted pAC50 on test set\n",
    "```\n",
    "\n",
    "After you have created or modified this `YAML` file to your liking, you can run the workflow with the below command either in a `bash` cell or in your command line:\n",
    "```\n",
    "openadmet anvil --recipe-path <your_file.yaml>\n",
    "```\n",
    "\n",
    "This may take 5-10 minutes to run, depending on the number of epochs, your hyperparameters (e.g. learning rate), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb012b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "openadmet anvil --recipe-path 2.1_anvil_lgbm.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3873eae",
   "metadata": {},
   "source": [
    "The outputs of the Anvil workflow are in `/anvil_training`:  \n",
    "- `/data` folder includes the split data, saved as `.csv`\n",
    "- `/recipe_components` folder contains the inputs from the `2.1_anvil_lgbm.yaml` file split by section\n",
    "- `cross_validation_metrics.json` is the cross validation metrics of the model saved as a `.json` file\n",
    "- `model.json` is the model's hyperparameters saved as a `.json` file\n",
    "- `regression_metrics.json` is the regression metrics saved as a `.json` file\n",
    "- `model.pkl` is the trained model saved as `.pkl` which can be loaded and used for predictions elsewhere\n",
    "- `cross_validation_regplot.png` is a plot of the cross validation metrics of the model\n",
    "- `anvil_recipe.yaml` is a copy of the input `.yaml`\n",
    "\n",
    "Here are the results of above trained LGBM model:\n",
    "\n",
    "<img src=\"./anvil_training/cross_validation_regplot.png\" alt=\"LGBM model results\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e72e2",
   "metadata": {},
   "source": [
    "## 4. Training a deep learning Chemprop model\n",
    "\n",
    "Here is a `YAML` file for training OpenADMET's ChemProp model. We are using the same ChEMBL PXR dataset. Be sure to note the different fields required for deep learning.\n",
    "\n",
    "\n",
    "```yaml\n",
    "# Section specifying input data\n",
    "data:\n",
    "  # Specify where anvil will run (inputs, outputs, etc.)\n",
    "  anvil_dir: demos/anvil_models/chemprop\n",
    "  # Specify the dataset file\n",
    "  resource: processed_data/processed_PXR_chembl.parquet\n",
    "  # TODO: add description of intake\n",
    "  type: intake\n",
    "  # COMMENT ONCE WE KNOW WHAT THIS IS\n",
    "  cat_entry: null\n",
    "  # Specify input column containing SMILES\n",
    "  input_col: OPENADMET_CANONICAL_SMILES\n",
    "  # Specify each (1+) of the target columns\n",
    "  target_cols:\n",
    "  - OPENADMET_LOGAC50\n",
    "\n",
    "# Additional metadata\n",
    "metadata:\n",
    "  authors: Cynthia Xu\n",
    "  biotargets:\n",
    "  - PXR\n",
    "  build_number: 0\n",
    "  description: basic regression using a ChemProp multitask task model\n",
    "  driver: pytorch\n",
    "  email: cynthia.xu@omsf.io\n",
    "  name: chemprop_pchembl\n",
    "  tag: chemprop-PXR-chembl\n",
    "  tags:\n",
    "  - openadmet\n",
    "  - test\n",
    "  - pchembl\n",
    "  version: v1\n",
    "\n",
    "# Section specifying training procedure\n",
    "procedure:\n",
    "  # Featurization specification\n",
    "  feat:\n",
    "    # Using the ChemPropFeaturizer (for ChemProp model)\n",
    "    # See openadmet.models.features\n",
    "    type: ChemPropFeaturizer\n",
    "    # No parameters passed\n",
    "    params: {}\n",
    "  \n",
    "  # Model specification\n",
    "  model:\n",
    "    # Indicate model type\n",
    "    # See openadmet.models.architecture\n",
    "    type: ChemPropMultiRegressorModel\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      depth: 3\n",
    "      ffn_hidden_dim: 1024\n",
    "      ffn_hidden_num_layers: 1\n",
    "      message_hidden_dim: 2048\n",
    "      dropout: 0.2\n",
    "      batch_norm: True\n",
    "      messages: bond\n",
    "      n_tasks: 1\n",
    "\n",
    "  # Specify data splits\n",
    "  split:\n",
    "    # Specify how data will be split\n",
    "    # See openadmet.models.split\n",
    "    type: ShuffleSplitter\n",
    "    # Specify split parameters\n",
    "    params:\n",
    "      random_state: 42\n",
    "      train_size: 0.7\n",
    "      val_size: 0.2 # For LGBM, set to 0\n",
    "      test_size: 0.1\n",
    "    \n",
    "  # Specify training configuration\n",
    "  train:\n",
    "    # Specify the trainer, here LightningTrainer as ChemProp is a PyTorch Lightning model\n",
    "    # See openadmet.models.trainer\n",
    "    type: LightningTrainer\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      accelerator: gpu\n",
    "      early_stopping: true\n",
    "      early_stopping_patience: 10\n",
    "      max_epochs: 20\n",
    "      monitor_metric: val_loss\n",
    "      use_wandb: false\n",
    "      # Don't blindly copy this over!\n",
    "      wandb_project: demos\n",
    "\n",
    "# Section specifying report generation\n",
    "report:\n",
    "  # Configure evaluation\n",
    "  eval:\n",
    "  # Generate regression metrics\n",
    "  - type: RegressionMetrics\n",
    "    params: {}\n",
    "  # Generate regression plots\n",
    "  - type: RegressionPlots\n",
    "    params:\n",
    "      axes_labels:\n",
    "      - True pChEMBL\n",
    "      - Predicted pChEMBL\n",
    "      max_val: 10\n",
    "      min_val: 3\n",
    "      pXC50: true\n",
    "      title: Multitask True vs Predicted pChEMBL on test set\n",
    "```\n",
    "\n",
    "This cell will also take around 20-40 minutes to run on a local CPU for a **single epoch**. The model results shown are with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "openadmet anvil --recipe-path 2.1_anvil_chemprop.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e5073",
   "metadata": {},
   "source": [
    "Here are the results of the ChemProp model:\n",
    "\n",
    "<img src=\"anvil_training_2025-07-14_86073d/cross_validation_regplot_OPENADMET_LOGAC50.png\" alt=\"ChemProp model results\" width=\"1000\"/>\n",
    "\n",
    "Congrats! You now know how to train models with the Anvil workflow. Explore our [model catalog](https://github.com/OpenADMET/openadmet-models/tree/2f58b521cdf122d8c929f6b64aead96d1378cd6f/openadmet/models) for other model architectures and featurizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5633b",
   "metadata": {},
   "source": [
    "✨✨✨✨✨✨✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openadmet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
