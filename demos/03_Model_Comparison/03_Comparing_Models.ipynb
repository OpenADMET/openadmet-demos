{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6fae82",
   "metadata": {},
   "source": [
    "# 3.0 Comparing Models\n",
    "### After Training your Models, What's Next?\n",
    "After training multiple models with Anvil, you will want to compare the performance across models in a robust way. We closely follow the guidlines laid out in [this paper](https://chemrxiv.org/engage/chemrxiv/article-details/672a91bd7be152b1d01a926b). Consider the below decision chart for helping figure out which models to compare:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"../../static/comparison_guidelines.png\" alt=\"Model comparison\" width=\"500\"/>.  \n",
    "</div>\n",
    "\n",
    "### Requirements\n",
    "For this demo, you will need:\n",
    "1. At least 2 models trained with the Anvil workflow.\n",
    "2. All models trained with same cross validation splits, e.g. 5 splits x 5 repeats\n",
    "## Overview\n",
    "This notebook will walk you through how to use the OpenADMET CLI to evaluate models that have been trained with the Anvil workflow. In this particular demo, we will compare the models we trained in `02_Training_Models.ipynb`.\n",
    "\n",
    "## Use Anvil to Compare Models\n",
    "As with training models with Anvil, comparing models is also a simple command with the following arguments:\n",
    "```bash\n",
    "    openadmet compare \\\n",
    "        --model-stats <path-1/cross_validation_metrics.json> \\ # this is the path to the cross_validation_metrics.json file output by anvil of your first model\n",
    "        --model-tag <a-tag-to-label-your-trained-model-1> \\ # this can be any moniker that is distinguishable for you\n",
    "        --task-name <name-of-task-1> \\ # this is the name of your target_cols from the anvil recipe.yaml\n",
    "        \\\n",
    "\n",
    "        --model-stats <path-2/cross_validation_metrics.json> \\ # corresponding info for your second model\n",
    "        --model-tag <a-tag-to-label-your-trained-model-2> \\\n",
    "        --task-name <name-of-task-2> \\\n",
    "\n",
    "        ... repeat this set of arguments for as many models as you want to compare\n",
    "\n",
    "\n",
    "        --output-dir <path-to-output-plots> \\ # this is an existing directory for your plot to export to\n",
    "        --report <whether-or-not-to-write-pdf-report>\n",
    "```\n",
    "**IMPORTANT NOTE** You can only compare models that have the same number of cross validation folds, e.g. a model with `5 splits x 2 repeats` can only be compared to another model that is also cross validated with `5 splits x 2 repeats`.\n",
    "For this demo, this command is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ab721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cynthiaxu/miniforge3/envs/demos/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Usage: openadmet compare [OPTIONS]\n",
      "Try 'openadmet compare --help' for help.\n",
      "\n",
      "Error: Invalid value for '--model-stats': Path '../../02_Model_Training/lgbm/cross_validation_metrics.json' does not exist.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'openadmet compare \\\\\\n    --model-stats ../../02_Model_Training/lgbm/cross_validation_metrics.json \\\\\\n    --model-tag lgbm \\\\\\n    --task-name OPENADMET_LOGAC50 \\\\\\n    --model-stats ../../02_Model_Training/chemprop/cross_validation_metrics.json \\\\\\n    --model-tag chemprop \\\\\\n    --task-name OPENADMET_LOGAC50 \\\\\\n    --model-stats ../../02_Model_Training/multitask/cross_validation_metrics.json \\\\\\n    --model-tag multitask \\\\\\n    --task-name OPENADMET_LOGAC50_cyp3a4 \\\\\\n    --output-dir model_comparisons/ \\\\\\n    --report True\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenadmet compare \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --model-stats ../../02_Model_Training/lgbm/cross_validation_metrics.json \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --model-tag lgbm \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --task-name OPENADMET_LOGAC50 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --model-stats ../../02_Model_Training/chemprop/cross_validation_metrics.json \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --model-tag chemprop \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --task-name OPENADMET_LOGAC50 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --model-stats ../../02_Model_Training/multitask/cross_validation_metrics.json \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --model-tag multitask \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --task-name OPENADMET_LOGAC50_cyp3a4 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --output-dir model_comparisons/ \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    --report True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/demos/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/demos/lib/python3.12/site-packages/IPython/core/magics/script.py:160\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/demos/lib/python3.12/site-packages/IPython/core/magics/script.py:348\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    345\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    346\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    347\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'openadmet compare \\\\\\n    --model-stats ../../02_Model_Training/lgbm/cross_validation_metrics.json \\\\\\n    --model-tag lgbm \\\\\\n    --task-name OPENADMET_LOGAC50 \\\\\\n    --model-stats ../../02_Model_Training/chemprop/cross_validation_metrics.json \\\\\\n    --model-tag chemprop \\\\\\n    --task-name OPENADMET_LOGAC50 \\\\\\n    --model-stats ../../02_Model_Training/multitask/cross_validation_metrics.json \\\\\\n    --model-tag multitask \\\\\\n    --task-name OPENADMET_LOGAC50_cyp3a4 \\\\\\n    --output-dir model_comparisons/ \\\\\\n    --report True\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "openadmet compare \\\n",
    "    --model-stats ../../02_Model_Training/lgbm/cross_validation_metrics.json \\\n",
    "    --model-tag lgbm \\\n",
    "    --task-name OPENADMET_LOGAC50 \\\n",
    "    --model-stats ../../02_Model_Training/chemprop/cross_validation_metrics.json \\\n",
    "    --model-tag chemprop \\\n",
    "    --task-name OPENADMET_LOGAC50 \\\n",
    "    --model-stats ../../02_Model_Training/multitask/cross_validation_metrics.json \\\n",
    "    --model-tag multitask \\\n",
    "    --task-name OPENADMET_LOGAC50_cyp3a4 \\\n",
    "    --output-dir model_comparisons/ \\\n",
    "    --report True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf35ff7",
   "metadata": {},
   "source": [
    "Now, in model comparisons, you should find these outputs:\n",
    "- `Levene.json` - file containing results of Levene test which assesses homogeneity of variances among groups\n",
    "- `Tukey_HSD.json` - file containing confidence intervals for Tukey HSD (honestly significant difference) test for pairwise comparisons between models\n",
    "- `anova.pdf` - ANOVA (analsyis of variance) plot showing whether each metric across all the compared models are statistically signficantly different; p-value ≤ 0.05\n",
    "- `mcs_plots.pdf`- multiple comparisons similarity plot where the color denotes effect size and asterisk annotations denote statistical significance\n",
    "- `mean_diffs.pdf`- plot of confidence intervals of the difference in mean performance between models; intervals that do not cross the zero line imply statistical significance\n",
    "- `normality_plots.pdf` - plots to show how normal the distribution of metrics are to check assumptions of parametric tests, e.g. ANOVA, etc.\n",
    "- `paired_plots.pdf` - plots to check pairwise relationships between metrics across the comparing models\n",
    "- `posthoc.pdf` - a file containing the tabulated Levene and Tukey HSD results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138d65b",
   "metadata": {},
   "source": [
    "### Interpreting the comparison plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1283cfb",
   "metadata": {},
   "source": [
    "✨✨✨✨✨✨✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
