{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fb9470",
   "metadata": {},
   "source": [
    "# 2.0 Model Training\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"static/anvil_diagram.png\" alt=\"Anvil diagram\" width=\"500\"/>  \n",
    "</div>\n",
    "\n",
    "## How the Anvil Infrastructure Works\n",
    "\n",
    "Anvil is our primary infrastructure for model training and evaluation, built to support scalable, reproducible, and rigorous development of ADMET prediction models.  \n",
    "\n",
    "Recognizing that building the best models requires training many variants, ensuring their reproducibility, and enabling robust performance comparisons, Anvil centers around a YAML-based recipe system.  \n",
    "\n",
    "These recipes allow users to specify model architectures and training procedures in a standardized, shareable format—minimizing code duplication while supporting both deep learning and traditional machine learning approaches.  \n",
    "\n",
    "Designed with both internal and external engagement in mind, Anvil aims to lower the barrier for outside users to adopt and fine-tune models by offering simple, transparent workflows. Long-term, it will serve as a foundation for broader community involvement and model reuse.\n",
    "\n",
    "### Requirements\n",
    "To run Anvil, you need:\n",
    "1. A dataset that has been processed with `01_Curate_ChEMBL_Data.ipynb`.  \n",
    "2. A `YAML` file with instructions for Anvil. We will show you how to create this file in this notebook.\n",
    "\n",
    "## Training a Model to Predict CYP3A4 Inhibition\n",
    "\n",
    "Now that we have a cleaned a dataset, we can train a model to predict **CYP3A4 inhibition**.  \n",
    "\n",
    "This notebook will walk you through how to run the Anvil model training workflow with the CYP3A4 data processed and cleaned in previous notebooks.\n",
    "\n",
    "## Creating the YAML file\n",
    "The heart of an anvil run is in its `YAML` configuration file. Here we specify nearly everything needed to:\n",
    "- load data\n",
    "- preprocess it\n",
    "- split the data appropriately into train/validation/test\n",
    "- featurize according to model selection\n",
    "- train the model\n",
    "- and, finally, validate on the test set (which generates performance metrics and plots)  \n",
    "\n",
    "We will walkthrough two `YAML` files: one for training a traditional machine learning model (`anvil_lgbm.yaml`) and one for training a deep learning model (`anvil_chemprop.yaml`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7a193",
   "metadata": {},
   "source": [
    "## Training a Traditional Machine Learning model: LightGBM \n",
    "\n",
    "Here is a `YAML` file for training a LightGBM (LGBM) model. We are using the previously curated CYP3A4 data from ChEMBL. Be sure to read through the comments (in green) to understand each field.  \n",
    "\n",
    "1. At a minimum, ensure `resource`, `input_col`, and `target_cols` are specified to match your dataset, as these will vary per dataset\n",
    "2. The `procedure` section may not need much modification, especially if not tweaking parameters, but look it over to make sure it’s sensible\n",
    "\n",
    "```yaml \n",
    "# This spection specifies the data that will be input into the model\n",
    "data:\n",
    "  # Specify the dataset file\n",
    "  resource: ../../01_Data_Curation/processed_data/processed_CYP3A4_inhibition.csv\n",
    "  type: intake\n",
    "  input_col: OPENADMET_CANONICAL_SMILES\n",
    "  # Specify each (1+) of the target columns, or the column that you're trying to predict\n",
    "  target_cols:\n",
    "  - OPENADMET_LOGAC50\n",
    "  dropna: true\n",
    "\n",
    "# Additional metadata\n",
    "# This should be descriptive as the tags in these fields will annotate downstream Anvil processes:\n",
    "# mainly, when you do model inference\n",
    "metadata:\n",
    "  authors: Your Name\n",
    "  email: youremail@email.com\n",
    "  biotargets:\n",
    "  - CYP3A4\n",
    "  build_number: 0\n",
    "  description: basic regression using a LightGBM model\n",
    "  driver: sklearn\n",
    "  name: lgbm_pchembl\n",
    "  tag: openadmet-chembl\n",
    "  tags:\n",
    "  - openadmet\n",
    "  - test\n",
    "  - pchembl\n",
    "  version: v1\n",
    "\n",
    "# Section specifying training procedure:\n",
    "# What model will you use?\n",
    "# What featurizers will the model use?\n",
    "# What hyperparameters will the mdoel use?\n",
    "procedure:\n",
    "# Featurization specification\n",
    "  feat:\n",
    "    # Using concatenated features, which combines multiple featurizers\n",
    "    # here we use DescriptorFeaturizer and FingerprintFeaturizer for 2D RDKit descriptors and ECFP4 fingerprints\n",
    "    # See openadmet.models.features \n",
    "    type: FeatureConcatenator\n",
    "    # Add parameters for the featurizer. Full description of the featurizer options are in Section 5.\n",
    "    params:\n",
    "      featurizers:\n",
    "        DescriptorFeaturizer:\n",
    "          descr_type: \"desc2d\"\n",
    "        FingerprintFeaturizer:\n",
    "          fp_type: \"ecfp:4\"\n",
    "  \n",
    "  # Model specification\n",
    "  model:\n",
    "    # Indicate model type\n",
    "    # See openadmet.models.architecture for all model types\n",
    "    type: LGBMRegressorModel\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      alpha: 0.005\n",
    "      learning_rate: 0.05\n",
    "      n_estimators: 500\n",
    "\n",
    "\n",
    "  # Specify data splits\n",
    "  split:\n",
    "    # Specify how data will be split\n",
    "    # See openadmet.models.split\n",
    "    type: ShuffleSplitter\n",
    "    # Specify split parameters\n",
    "    params:\n",
    "      random_state: 42\n",
    "      train_size: 0.8\n",
    "      val_size: 0.0 # For LGBM, no validation set is needed\n",
    "      test_size: 0.2 # If you want to compare tree-based models with Dl models later, the test sizes should match\n",
    "    \n",
    "  # Specify training configuration\n",
    "  train:\n",
    "    # Specify the trainer, here SKLearnBasicTrainer as model has an sklearn interface\n",
    "    # could also use SKLearnGridSearchTrainer for hyperparameter tuning\n",
    "    type: SKLearnBasicTrainer\n",
    "\n",
    "\n",
    "# Section specifying report generation\n",
    "# What cross validation splits will you use?\n",
    "# You can also specify the min and max values of your plots\n",
    "report:\n",
    "  # Configure evaluation\n",
    "  eval:\n",
    "  # Generate regression metrics\n",
    "  - type: RegressionMetrics\n",
    "    params: {}\n",
    "  # Generate regression plots & do cross validation\n",
    "  - type: SKLearnRepeatedKFoldCrossValidation\n",
    "    params:\n",
    "      axes_labels:\n",
    "      - True pAC50\n",
    "      - Predicted pAC50\n",
    "      max_val: 10\n",
    "      min_val: 3\n",
    "      pXC50: true\n",
    "      n_splits: 5\n",
    "      n_repeats: 5\n",
    "      title: True vs Predicted pAC50 on test set\n",
    "\n",
    "```\n",
    "\n",
    "After you have created or modified this `YAML` file to your liking, you can run the workflow with the below command either in a `bash` cell or in your command line:\n",
    "```\n",
    "  openadmet anvil --recipe-path <your_file.yaml> --output-dir <output folder name>\n",
    "```\n",
    "\n",
    "This may take 5-10 minutes to run, depending on the number of epochs, your hyperparameters (e.g. learning rate), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb012b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export OADMET_NO_RICH_LOGGING=1\n",
    "\n",
    "openadmet anvil --recipe-path anvil_lgbm.yaml --output-dir lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3873eae",
   "metadata": {},
   "source": [
    "The outputs of the Anvil workflow are in `/anvil_training`:  \n",
    "- `/data` folder includes the split data, saved as `.csv`\n",
    "- `/recipe_components` folder contains the inputs from the `2.1_anvil_lgbm.yaml` file split by section\n",
    "- `cross_validation_metrics.json` is the cross validation metrics of the model saved as a `.json` file\n",
    "- `model.json` is the model's hyperparameters saved as a `.json` file\n",
    "- `regression_metrics.json` is the regression metrics saved as a `.json` file\n",
    "- `model.pkl` is the trained model saved as `.pkl` which can be loaded and used for predictions elsewhere\n",
    "- `cross_validation_regplot.png` is a plot of the cross validation metrics of the model\n",
    "- `anvil_recipe.yaml` is a copy of the input `.yaml`\n",
    "\n",
    "Here are the results of above trained LGBM model:\n",
    "\n",
    "<img src=\"lgbm/cross_validation_regplot.png\" alt=\"LGBM model results\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e72e2",
   "metadata": {},
   "source": [
    "## Training a Deep Learning model: ChemProp\n",
    "\n",
    "Our framework is also capable of training deep learning models, but for ease of demonstration on CPU, we won't actually train the model here. We recommend training deep learning models on GPU.  \n",
    "\n",
    "As an example, we've provided an already trained ChemProp model for you to look at for this demo.\n",
    "\n",
    "Here is a `YAML` file (`anvil_chemprop.yaml`) for training OpenADMET's ChemProp model. We are using the same ChEMBL CYP3A4 dataset. Be sure to note the different fields required for deep learning.\n",
    "\n",
    "```yaml\n",
    "# This spection specifies the input data \n",
    "data:\n",
    "  # Specify the dataset file\n",
    "  resource: ../01_Data_Curation/processed_data/processed_CYP3A4_inhibition.csv\n",
    "  type: intake\n",
    "  input_col: OPENADMET_CANONICAL_SMILES\n",
    "  # Specify each (1+) of the target columns, or the column that you're trying to predict\n",
    "  target_cols:\n",
    "  - OPENADMET_LOGAC50\n",
    "\n",
    "\n",
    "# Additional metadata\n",
    "metadata:\n",
    "  authors: Your Name\n",
    "  email: youremail@mail.com\n",
    "  biotargets:\n",
    "  - CYP3A4\n",
    "  build_number: 0\n",
    "  description: basic regression using a ChemProp multitask task model\n",
    "  driver: pytorch\n",
    "  name: chemprop_pchembl\n",
    "  tag: chemprop-CYP3A4-chembl\n",
    "  tags:\n",
    "  - openadmet\n",
    "  - test\n",
    "  version: v1\n",
    "\n",
    "# Section specifying training procedure\n",
    "procedure:\n",
    "  # Featurization specification\n",
    "  feat:\n",
    "    # Using the ChemPropFeaturizer (for ChemProp model)\n",
    "    # See openadmet.models.features\n",
    "    type: ChemPropFeaturizer\n",
    "    # No parameters passed\n",
    "    params: {}\n",
    "  \n",
    "  # Model specification\n",
    "  model:\n",
    "    # Indicate model type\n",
    "    # See openadmet.models.architecture\n",
    "    type: ChemPropModel\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      depth: 4\n",
    "      ffn_hidden_dim: 1024\n",
    "      ffn_hidden_num_layers: 4\n",
    "      message_hidden_dim: 2048\n",
    "      dropout: 0.2\n",
    "      batch_norm: True\n",
    "      messages: bond\n",
    "      n_tasks: 1 # Number of tasks should match the number of target columns\n",
    "      from_chemeleon: False\n",
    "\n",
    "  # Specify data splits\n",
    "  split:\n",
    "    # Specify how data will be split\n",
    "    # See openadmet.models.split\n",
    "    type: ShuffleSplitter\n",
    "    # Specify split parameters\n",
    "    params:\n",
    "      random_state: 42\n",
    "      train_size: 0.7\n",
    "      val_size: 0.1\n",
    "      test_size: 0.2\n",
    "    \n",
    "  # Specify training configuration\n",
    "  train:\n",
    "    # Specify the trainer, here LightningTrainer as ChemProp is a PyTorch Lightning model\n",
    "    # See openadmet.models.trainer\n",
    "    type: LightningTrainer\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      accelerator: gpu\n",
    "      early_stopping: true\n",
    "      early_stopping_patience: 10\n",
    "      early_stopping_mode: min\n",
    "      early_stopping_min_delta: 0.001\n",
    "      max_epochs: 50\n",
    "      monitor_metric: val_loss\n",
    "      use_wandb: false\n",
    "      wandb_project: demos # Specify wandb project name according to guidelines\n",
    "\n",
    "# Section specifying report generation\n",
    "report:\n",
    "  # Configure evaluation\n",
    "  eval:\n",
    "  # Generate regression metrics\n",
    "  - type: RegressionMetrics\n",
    "    params: {}\n",
    "  # Generate regression plots & do cross validation\n",
    "  - type: PytorchLightningRepeatedKFoldCrossValidation\n",
    "    params:\n",
    "      axes_labels:\n",
    "      - True LogAC50\n",
    "      - Predicted LogAC50\n",
    "      n_repeats: 5\n",
    "      n_splits: 5\n",
    "      random_state: 42\n",
    "      pXC50: true\n",
    "      title: True vs Predicted LogAC50 on test set\n",
    "```\n",
    "\n",
    "The command is\n",
    "```\n",
    "openadmet anvil --recipe-path anvil_chemprop.yaml --output-dir chemprop\n",
    "```\n",
    "We recommend training deep learning models on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0046aa-e3b8-4aa5-b568-5f19b3fc2de2",
   "metadata": {},
   "source": [
    "The results of a pre-trained version we provide are shown here\n",
    "\n",
    "\n",
    "<img src=\"./chemprop/cross_validation_regplot.png\" alt=\"ChemProp model results\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df8f65",
   "metadata": {},
   "source": [
    "## Training a Multitask Deep Learning Model: ChemProp\n",
    "\n",
    "Similarly to the single task deep learning example above, we've gone ahead and trained this model for you. We recommend training deep learning models on GPU.\n",
    "\n",
    "There may be instances where you will want to train a model to predict compound activity on multiple protein targets.\n",
    "\n",
    "For example, you may have endpoints that share a biochemical pathway such that activity on one is thought to be somewhat correlated to the other. \n",
    "\n",
    "It would thus be useful to train a multitask model on multiple targets. The `YAML` file example shown below is `anvil_multitask.yaml`.\n",
    "\n",
    "```yaml\n",
    "# Section specifying input data\n",
    "data:\n",
    "  # Specify the dataset file, can be S3 path etc.\n",
    "  resource:  ../01_Data_Curation/processed_data/multitask.parquet\n",
    "  # must be intake\n",
    "  type: intake\n",
    "  # Specify input column containing SMILES\n",
    "  input_col: OPENADMET_CANONICAL_SMILES\n",
    "  # Specify whether or not to drop NaN data rows\n",
    "  dropna: False\n",
    "  # Specify each (1+) of the target columns\n",
    "  target_cols:\n",
    "  - OPENADMET_LOGAC50_cyp2j2\n",
    "  - OPENADMET_LOGAC50_cyp3a4\n",
    "  - OPENADMET_LOGAC50_cyp1a2\n",
    "  - OPENADMET_LOGAC50_pxr\n",
    "  - OPENADMET_LOGAC50_cyp2d6\n",
    "  - OPENADMET_LOGAC50_cyp2c9\n",
    "  - OPENADMET_LOGAC50_ahr\n",
    "\n",
    "# Additional metadata\n",
    "metadata:\n",
    "  authors: Your Name\n",
    "  email: youremail@mail.com\n",
    "  biotargets:\n",
    "  - CYP3A4\n",
    "  - CYP2J2\n",
    "  - CYP1A2\n",
    "  - CYP2D6\n",
    "  - CYP2C9\n",
    "  - PXR\n",
    "  - AHR\n",
    "  build_number: 0\n",
    "  description: basic regression using a ChemProp multitask task model\n",
    "  driver: pytorch\n",
    "  name: chemprop_pchembl\n",
    "  tag: chemprop\n",
    "  tags:\n",
    "  - openadmet\n",
    "  - test\n",
    "  - chemprop\n",
    "  version: v1\n",
    "\n",
    "# Section specifying training procedure\n",
    "procedure:\n",
    "  # Featurization specification\n",
    "  feat:\n",
    "    # Using the ChemPropFeaturizer (for ChemProp model)\n",
    "    # See openadmet.models.features\n",
    "    type: ChemPropFeaturizer\n",
    "    # No parameters passed\n",
    "    params: {}\n",
    "  \n",
    "  # Model specification\n",
    "  model:\n",
    "    # Indicate model type\n",
    "    # See openadmet.models.architecture\n",
    "    type: ChemPropModel\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      depth: 4\n",
    "      ffn_hidden_dim: 1024\n",
    "      ffn_hidden_num_layers: 4\n",
    "      message_hidden_dim: 2048\n",
    "      dropout: 0.2\n",
    "      batch_norm: True\n",
    "      messages: bond\n",
    "      n_tasks: 7 # Number of tasks should match the number of target columns\n",
    "      from_chemeleon: False\n",
    "\n",
    "  # Specify data splits\n",
    "  split:\n",
    "    # Specify how data will be split, can be ShuffleSplitter, ScaffoldSplitter, etc.\n",
    "    # See openadmet.models.split\n",
    "    type: ShuffleSplitter\n",
    "    # Specify split parameters\n",
    "    params:\n",
    "      random_state: 42\n",
    "      train_size: 0.7\n",
    "      val_size: 0.1\n",
    "      test_size: 0.2\n",
    "    \n",
    "  # Specify training configuration\n",
    "  train:\n",
    "    # Specify the trainer, here LightningTrainer as ChemProp is a PyTorch Lightning model\n",
    "    # See openadmet.models.trainer\n",
    "    type: LightningTrainer\n",
    "    # Specify model parameters\n",
    "    params:\n",
    "      accelerator: gpu\n",
    "      early_stopping: true\n",
    "      early_stopping_patience: 10\n",
    "      early_stopping_mode: min\n",
    "      early_stopping_min_delta: 0.001\n",
    "      max_epochs: 50\n",
    "      monitor_metric: val_loss\n",
    "      use_wandb: false\n",
    "      wandb_project: demos # Specify wandb project name according to guidelines\n",
    "\n",
    "# Section specifying report generation\n",
    "report:\n",
    "  # Configure evaluation\n",
    "  eval:\n",
    "  # Generate regression metrics\n",
    "  - type: RegressionMetrics\n",
    "    params: {}\n",
    "  # Generate regression plots & do cross validation\n",
    "  - type: PytorchLightningRepeatedKFoldCrossValidation\n",
    "    params:\n",
    "      axes_labels:\n",
    "      - True LogAC50\n",
    "      - Predicted LogAC50\n",
    "      n_repeats: 5\n",
    "      n_splits: 5\n",
    "      random_state: 42\n",
    "      pXC50: true\n",
    "      title: Multitask True vs Predicted LogAC50 on test set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da263b6-d2ac-4d10-8f58-76473f5e83b9",
   "metadata": {},
   "source": [
    "The results of a pre-trained version we provide are shown here\n",
    "\n",
    "\n",
    "<img src=\"./multitask/cross_validation_regplot_OPENADMET_LOGAC50_cyp3a4.png\" alt=\"ChemProp model results\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e5073",
   "metadata": {},
   "source": [
    "We will examine the full results of these models in `03_Model_Comparison`.\n",
    "\n",
    "Congrats! You now know how to train models with the Anvil workflow. Explore our [model catalog](https://github.com/OpenADMET/openadmet-models/tree/main/openadmet/models) for other model architectures and featurizers.\n",
    "\n",
    "\n",
    "**Now let's compare the performance of our models!**\n",
    "\n",
    "~ End of `02_Training_Models` ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
