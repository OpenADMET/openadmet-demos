{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de6fae82",
   "metadata": {},
   "source": [
    "# 3.0 Comparing Models\n",
    "## After Training your Models, What's Next?\n",
    "After training multiple models with Anvil, you will want to compare the performance across models in a robust way. Comparing models properly and with appropriate statistical rigour is very important for establishing which models you should be deploying in production for your endpoints.  \n",
    "\n",
    "Just comparing a single number (say a metric like MAE) from a single training run is not a robust way to compare models due to the possiblity of over-optimising to a single split.  \n",
    "\n",
    "To properly compare models, you should sample from the performance distribution of your model using a technique like cross validation. We closely follow the guidlines laid out in [this paper](https://chemrxiv.org/engage/chemrxiv/article-details/672a91bd7be152b1d01a926b). Consider the below decision chart for helping figure out how to compare your models:\n",
    "\n",
    "<img src=\"./static/comparison_guidelines.png\" alt=\"model comparison\" width=\"500\"/>\n",
    "\n",
    "### Requirements\n",
    "For this demo, you will need:\n",
    "1. At least 2 models trained with the Anvil workflow.\n",
    "2. All models trained with same cross validation splits, e.g. 5 splits x 5 repeats\n",
    "## Overview\n",
    "This notebook will walk you through how to use the OpenADMET CLI to evaluate models that have been trained with the Anvil workflow. In this particular demo, we will compare the models we trained in `02_Training_Models.ipynb`.\n",
    "\n",
    "## Use Anvil to Compare Models\n",
    "As with training models with Anvil, comparing models is also a simple command with the following arguments:\n",
    "```bash\n",
    "    openadmet compare \\\n",
    "        --model-stats <path-1/cross_validation_metrics.json> \\ # this is the path to the cross_validation_metrics.json file output by anvil of your first model\n",
    "        --model-tag <a-tag-to-label-your-trained-model-1> \\ # this can be any moniker that is distinguishable for you\n",
    "        --task-name <name-of-task-1> \\ # this is the name of your target_cols from the anvil recipe.yaml\n",
    "        \\\n",
    "\n",
    "        --model-stats <path-2/cross_validation_metrics.json> \\ # corresponding info for your second model\n",
    "        --model-tag <a-tag-to-label-your-trained-model-2> \\\n",
    "        --task-name <name-of-task-2> \\\n",
    "\n",
    "        ... repeat this set of arguments for as many models as you want to compare\n",
    "\n",
    "\n",
    "        --output-dir <path-to-output-plots> \\ # this is an existing directory for your plot to export to\n",
    "        --report <whether-or-not-to-write-pdf-report>\n",
    "```\n",
    "**IMPORTANT NOTE** You can only compare models that have the same number of cross validation folds, e.g. a model with `5 splits x 2 repeats` can only be compared to another model that is also cross validated with `5 splits x 2 repeats`.  \n",
    "\n",
    "For this demo, this command is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ab721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cynthiaxu/miniforge3/envs/demos/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/Users/cynthiaxu/miniforge3/envs/demos/lib/python3.12/site-packages/openadmet/models/comparison/posthoc.py:476: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '***' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  significance[(hsd.pvalue < self.sig_levels[2]) & (hsd.pvalue >= 0)] = \"***\"\n",
      "/Users/cynthiaxu/miniforge3/envs/demos/lib/python3.12/site-packages/openadmet/models/comparison/posthoc.py:476: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '***' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  significance[(hsd.pvalue < self.sig_levels[2]) & (hsd.pvalue >= 0)] = \"***\"\n",
      "/Users/cynthiaxu/miniforge3/envs/demos/lib/python3.12/site-packages/openadmet/models/comparison/posthoc.py:476: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '***' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  significance[(hsd.pvalue < self.sig_levels[2]) & (hsd.pvalue >= 0)] = \"***\"\n",
      "/Users/cynthiaxu/miniforge3/envs/demos/lib/python3.12/site-packages/openadmet/models/comparison/posthoc.py:476: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '***' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  significance[(hsd.pvalue < self.sig_levels[2]) & (hsd.pvalue >= 0)] = \"***\"\n",
      "/Users/cynthiaxu/miniforge3/envs/demos/lib/python3.12/site-packages/openadmet/models/comparison/posthoc.py:476: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '***' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  significance[(hsd.pvalue < self.sig_levels[2]) & (hsd.pvalue >= 0)] = \"***\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levene's test results\n",
      "-------------------------\n",
      "+--------------+-------------+-----------+-----------+-------------+\n",
      "|          mse |         mae |        r2 |      ktau |   spearmanr |\n",
      "|--------------+-------------+-----------+-----------+-------------|\n",
      "| 15.3368      | 9.0868      | 2.52168   | 2.8462    |    1.8035   |\n",
      "|  2.82785e-06 | 0.000302768 | 0.0873985 | 0.0646163 |    0.172083 |\n",
      "+--------------+-------------+-----------+-----------+-------------+\n",
      "\n",
      "Tukey's HSD results\n",
      "-------------------------\n",
      "+--------------------+-----------+------------+-------------+-------------+\n",
      "| method             | metric    |      value |   errorbars |     p-value |\n",
      "|--------------------+-----------+------------+-------------+-------------|\n",
      "| lgbm-chemprop      | mse       | -0.0448613 |   0.0235377 | 6.0121e-05  |\n",
      "| lgbm-multitask     | mse       |  0.0692462 |   0.0235377 | 2.78765e-09 |\n",
      "| chemprop-multitask | mse       |  0.114107  |   0.0235377 | 0           |\n",
      "| lgbm-chemprop      | mae       | -0.0285522 |   0.0098576 | 4.42271e-09 |\n",
      "| lgbm-multitask     | mae       |  0.0554412 |   0.0098576 | 0           |\n",
      "| chemprop-multitask | mae       |  0.0839934 |   0.0098576 | 0           |\n",
      "| lgbm-chemprop      | r2        |  0.0568517 |   0.0268932 | 9.21074e-06 |\n",
      "| lgbm-multitask     | r2        |  0.147037  |   0.0268932 | 0           |\n",
      "| chemprop-multitask | r2        |  0.0901851 |   0.0268932 | 4.12251e-11 |\n",
      "| lgbm-chemprop      | ktau      |  0.0184267 |   0.0106302 | 0.000264308 |\n",
      "| lgbm-multitask     | ktau      |  0.0807836 |   0.0106302 | 0           |\n",
      "| chemprop-multitask | ktau      |  0.0623569 |   0.0106302 | 0           |\n",
      "| lgbm-chemprop      | spearmanr |  0.0213372 |   0.012386  | 0.000289132 |\n",
      "| lgbm-multitask     | spearmanr |  0.0997481 |   0.012386  | 0           |\n",
      "| chemprop-multitask | spearmanr |  0.0784109 |   0.012386  | 0           |\n",
      "+--------------------+-----------+------------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export OADMET_NO_RICH_LOGGING=1\n",
    "\n",
    "openadmet compare \\\n",
    "    --model-stats ../02_Model_Training/lgbm/cross_validation_metrics.json \\\n",
    "    --model-tag lgbm \\\n",
    "    --task-name OPENADMET_LOGAC50 \\\n",
    "    --model-stats ../02_Model_Training/chemprop/cross_validation_metrics.json \\\n",
    "    --model-tag chemprop \\\n",
    "    --task-name OPENADMET_LOGAC50 \\\n",
    "    --model-stats ../02_Model_Training/multitask/cross_validation_metrics.json \\\n",
    "    --model-tag multitask \\\n",
    "    --task-name OPENADMET_LOGAC50_cyp3a4 \\\n",
    "    --output-dir model_comparisons/ \\\n",
    "    --report True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf35ff7",
   "metadata": {},
   "source": [
    "Now, in model comparisons, you should find these outputs:\n",
    "- `Levene.json` - file containing results of Levene test which assesses homogeneity of variances among groups\n",
    "- `Tukey_HSD.json` - file containing confidence intervals for Tukey HSD (honestly significant difference) test for pairwise comparisons between models\n",
    "- `anova.pdf` - ANOVA (analsyis of variance) plot showing whether each metric across all the compared models are statistically signficantly different; p-value â‰¤ 0.05\n",
    "- `mcs_plots.pdf`- multiple comparisons similarity plot where the color denotes effect size and asterisk annotations denote statistical significance\n",
    "- `mean_diffs.pdf`- plot of confidence intervals of the difference in mean performance between models; intervals that do not cross the zero line imply statistical significance\n",
    "- `normality_plots.pdf` - plots to show how normal the distribution of metrics are to check assumptions of parametric tests, e.g. ANOVA, etc.\n",
    "- `paired_plots.pdf` - plots to check pairwise relationships between metrics across the comparing models\n",
    "- `posthoc.pdf` - a file containing the tabulated Levene and Tukey HSD results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138d65b",
   "metadata": {},
   "source": [
    "## Interpreting the comparison plots\n",
    "\n",
    "Let's walkthrough how you might interpret some of these generated plots.\n",
    "\n",
    "### ANOVA\n",
    "First, consider the `anova.pdf`. The analysis of variance (ANOVA) shows whether each performance metric is statistically different from the others, i.e. p-value â‰¤ 0.05.  \n",
    "\n",
    "**NOTE**: The models plotted in blue are the best for each metric.\n",
    "\n",
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "<img src=\"static/anova_mse_mae.png\" alt=\"ANOVA\" width=\"500\"/>.  \n",
    "<img src=\"static/anova_r2_ktau.png\" alt=\"ANOVA\" width=\"500\"/>.  \n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"static/anova_spearmanr.png\" alt=\"ANOVA\" width=\"500\"/>.  \n",
    "</div>\n",
    "\n",
    "From these plots, we see that the multitask ChemProp model has the best mean squared error (MSE) and mean absolute error (MAE), meaning that the multitask model out performs the LGBM and single task ChemProp model in predicting values that are close to the target values in the test set (on average)  \n",
    "\n",
    "However, the LGBM model has the highest:\n",
    "- $R^2$, which gives an idea of how well the variance in the actual values is explained by the model, aka goodness-of-fit.  \n",
    "- Kendall's $\\tau$, which measures how similar the rank order of predictions is to the rank order of true values.  \n",
    "- Spearman's $\\rho$, which also measures ranking, but more specifically, whether larger true values tend to correspend to larger predictions, even if not linearly.\n",
    "\n",
    "Thus depending on your use-case, you may prefer a model with higher numerical accuracy (the multitask ChemProp model) for more accurate activity predictions OR you may prefer a model that is better at predicting ranking of activities (the LGBM model).\n",
    "\n",
    "### Multiple Comparison Similarity\n",
    "These heatmaps visualize how similar or different the performance metrics (MSE, MAE, $R^2$, Kendall's $\\tau$, Spearman's $\\rho$) are from each other pairwise across different models.  \n",
    "\n",
    "Larger values are more similar and smaller/negative values mean are more different.  \n",
    "\n",
    "From these heatmaps, we can see that despite the LGBM model appearing to have better $R^2$, Kendall's $\\tau$, and Spearman's $\\rho$ from the ANOVA plots, the multitask model actually performs very similarly to the LGBM model.  \n",
    "\n",
    "<div>\n",
    "<img src=\"static/msc_plots.png\" alt=\"MCS\" width=\"1200\"/>.  \n",
    "</div>\n",
    "\n",
    "### Mean Differences\n",
    "These plots are another visualization of pairwise differences between model means that show the **magnitude** and **direction** in difference with 95% confidence intervals.  \n",
    "\n",
    "Each point is the difference in means between the two compared models, e.g. `chemprop-multitask`.\n",
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "<img src=\"static/mean_diffs_1.png\" alt=\"Mean Diff\" width=\"600\"/>.  \n",
    "<img src=\"static/mean_diffs_2.png\" alt=\"Mean Diff\" width=\"600\"/>.  \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Normality Plots\n",
    "These plots check the normality of the distribution of your metrics as sampled from cross validation. Most ANOVA and t-tests assume residuals are normally distributed.  \n",
    "\n",
    "With these plots, we are able to gauge the reliability of our parametric tests by checking the normality. Points that lie close the line are normally distributed, but in this case, we see some curving of the points from the line, implying that the residuals are skewed.\n",
    "<div>\n",
    "<img src=\"static/normality_plots.png\" alt=\"MCS\" width=\"900\"/>.  \n",
    "</div>\n",
    "\n",
    "### Paired plots\n",
    "Paired plots also visualize pairwise comparisons of our metrics between models on each cross validation fold. This can be a useful view for seeing how each fold performed between models.\n",
    "<div>\n",
    "<img src=\"static/paired_plots.png\" alt=\"MCS\" width=\"\\900\"/>.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1283cfb",
   "metadata": {},
   "source": [
    "Moving forward, we will use our `LGBM` model due to its superior ranking abilities and for ease of inference on a CPU.\n",
    "\n",
    "**Now let's take it up a notch by training a model ensemble!**\n",
    "\n",
    "~ End of `03_Comparing_Models` ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
